{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 뉴스 기사 내용 유사도 확인하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> 컴퓨터 공학과 2021105662 홍예림 </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 주제 선정 이유\n",
    "    \n",
    "    뉴스는 신속하게 정치, 사회, 경제 등 여러 분야의 새로운 정보를 얻을 수 있어서 오랫동안 많은 사람이 애용해왔다. 사람들이 종이신문보다 언론 사이트를 더 많이 찾아보게 되면서 문제가 생겨났다. 언론 사이트는 방문자 수가 많을수록 광고를 통한 수익도 상승하기 때문에 비슷한 내용의 기사를 여러 개 작성하여 낚시성의 제목으로 포털 사이트에 노출한다. 그래서 정보를 얻기 위해 뉴스 기사를 읽는다면 계속해서 비슷한 내용의 뉴스 기사만 읽게 된다. 지금까지 인터넷 뉴스를 읽은 경험에 따르면 다른 분야보다 특히 더 연예계, 정치, 경제 분야에서 비슷한 내용의 기사들이 매일 쏟아져 나온다. 최근 네이버의 실시간 검색어가 사라지면서 더 자극적이고 낚시성이 높은 제목을 붙인 기사들이 많아졌다. 하지만 기사 내용은 다른 기사들과 다를 바가 없다는 것을 계속해서 느껴왔다. 그래서 이를 해결하고자 텀 프로젝트 주제로 선정하게 되었다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 주제 정의\n",
    "    \n",
    "    Term 프로젝트를 통해 여러 언론 사이트에서 비슷한 내용의 뉴스 기사를 올리는 것이 사실인지 확인하고자 한다. 하나의 키워드를 골라 네이버에서 검색하면 나오는 오늘 발행된 뉴스 기사를 네이버 오픈 API를 이용하여 크롤링한다. 그다음, 뉴스 기사 제목과 url을 json 파일로 저장한 뒤 url을 하나씩 읽어와서 뉴스 기사 사이트에 있는 기사 부분만 beautifulsoup를 이용해 크롤링한다. 이후 konlpy를 이용해 형태소 분석을 해서 뉴스 기사에 있는 명사들을 json 파일로 저장한다. 검색된 뉴스 기사 중 하나를 골라 전체 뉴스 기사와 내용의 유사성을 TF-IDF를 이용해 증명할 예정이다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 인터넷을 통한 데이터 획득 - 1/2\n",
    "    \n",
    "   #### 1번 과정)  네이버 api를 이용하여 키워드를 검색하여 나온 뉴스 기사들의 url과 제목을 수집하여 json 파일로 저장\n",
    "    네이버 오픈 API를 이용해 네이버 뉴스 검색 결과를 받아온다. 비로그인 오픈 API이므로 GET으로 호출할 때 때 HTTP Header에 애플리케이션 등록 시 발급받은 Client ID와 Client Secret 값을 입력한다. 키워드는 사용자의 입력을 받아 설정한다. 키워드를 검색하여 나온 네이버 뉴스 검색 결과를 받아오면, 제목과, url, original url, 언론 사이트에서 제공하는 기사 요약, 발행된 날짜를 얻을 수 있다.\n",
    "    link가 네이버 뉴스와 연동되는 링크인 news.naver을 포함하고 있으면 뉴스 기사를 저장한다. 이 데이터 중에서 오늘 발행된 기사만 고르기 위해 datetime 모듈을 호출해서 현재 날짜를 받아온다. 네이버 오픈 API에서 받아온 날짜 데이터 순서는 요일, 일, 월, 년도이고, datetime 모듈을 통해 받아온 날짜 데이터 순서는 년도, 월, 일이다. 네이버 오픈 API로 받아온 날짜 데이터를 수정해서 datetime 모듈의 날짜 데이터 순서에 맞춘다. 네이버 오픈 API에서 얻는 발행된 날짜를 오늘 날짜와 비교하여 오늘 발행된 데이터들만 수집한다. 또한 오늘 안에 발행된 기사 중에서도 검색한 시간과 가장 근접한 시간대에 발행된 기사만 모으기 위해 뉴스 기사를 100개만 저장한다. 키워드로 검색하여 나온 뉴스 기사중에서 오늘 발행된 뉴스 기사가 없으면 에러 코드인 404를 리턴한다.\n",
    "    json 모듈을 이용해 기사 번호와 제목, url을 json 파일로 저장한다. 네이버 오픈 API로 받아온 데이터를 naver_api_original_data.json로 저장하고, 받아온 데이터 중에서 필요한 데이터만 추출해서 naver_api_result.json로 저장한다.  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) 네이버 api를 이용하여 키워드를 검색하여 나온 뉴스 기사들의 url과 제목을 수집하여 json 파일로 저장 #\n",
    "\n",
    "\n",
    "import urllib.request\n",
    "import json\n",
    "from datetime import datetime\n",
    "import codecs\n",
    "\n",
    "\n",
    "class LinkCollector:\n",
    "\n",
    "    # 네이버 뉴스 검색 결과를 출력해주는 네이버 API #\n",
    "\n",
    "    _CLIENT_ID = \"hqKMORcNCRx2jN3kSF6J\"     # 애플리케이션 등록 시 발급받은 client id 값 #\n",
    "    _CLIENT_SECRET = \"PbexHJlugP\"   # 애플리케이션 등록 시 발급받은 client secret 값 #\n",
    "    _DISPLAY = 100\n",
    "\n",
    "    # HTTP Header에 애플리케이션 등록 시 발급받은 Client ID와 Client Secret 값을 같이 전송하면 api 활용 가능 #\n",
    "\n",
    "    def __init__(self, word, date):\n",
    "        self.search_word = urllib.parse.quote(word)\n",
    "        self.date = date\n",
    "        self.start = 1\n",
    "        self.temp_url = \"https://openapi.naver.com/v1/search/news?query={0}&start={1}&display={2}&sort=date\"\n",
    "        self.total = 0\n",
    "        self._total()\n",
    "        self.count = 0\n",
    "        self.filename = './naver_api_result.json'\n",
    "        self.testfile = './naver_api_original_data.json'\n",
    "        self.result = dict()    # 뉴스 기사들의 번호, 제목, url을 저장하는 딕셔너리 #\n",
    "        self.navertest = []     # 네이버 api를 이용해 받아온 데이터 원본을 저장하는 딕셔너리#\n",
    "        self.collect_link()\n",
    "        self.save_to_file()\n",
    "\n",
    "    def _total(self):\n",
    "        url = self.temp_url.format(self.search_word, self.start, LinkCollector._DISPLAY)\n",
    "        request = urllib.request.Request(url)\n",
    "        request.add_header(\"X-Naver-Client-Id\", LinkCollector._CLIENT_ID)\n",
    "        request.add_header(\"X-Naver-Client-Secret\", LinkCollector._CLIENT_SECRET)\n",
    "        response = urllib.request.urlopen(request)\n",
    "        rescode = response.getcode()\n",
    "        try:\n",
    "                    response = urllib.request.urlopen(request)\n",
    "                    rescode = response.getcode()\n",
    "\n",
    "                    if rescode == 200:\n",
    "                        response_body = response.read()\n",
    "                        r = response_body.decode('utf-8')\n",
    "                        r = json.loads(r)\n",
    "                        self.total = r.get('total')\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        # 에러 코드 확인 #\n",
    "\n",
    "                        print(\"Error Code:\" + rescode)\n",
    "                        self.total = 0\n",
    "\n",
    "        except:\n",
    "            return\n",
    "\n",
    "    def collect_link(self):\n",
    "        is_stop = False\n",
    "\n",
    "        for i in range(0, int(self.total / 100)):\n",
    "            self.start = i * LinkCollector._DISPLAY + 1\n",
    "            url = self.temp_url.format(self.search_word, self.start, LinkCollector._DISPLAY)\n",
    "\n",
    "            request = urllib.request.Request(url)\n",
    "            request.add_header(\"X-Naver-Client-Id\", LinkCollector._CLIENT_ID)\n",
    "            request.add_header(\"X-Naver-Client-Secret\", LinkCollector._CLIENT_SECRET)\n",
    "            try:\n",
    "                response = urllib.request.urlopen(request)\n",
    "                rescode = response.getcode()\n",
    "\n",
    "                if rescode == 200:\n",
    "                    response_body = response.read()\n",
    "                    r = response_body.decode('utf-8')\n",
    "                    r = json.loads(r)\n",
    "\n",
    "                    for v in r.get('items'):\n",
    "                        self.navertest.append(v)\n",
    "                        title = '{0}'.format(v.get('title'))     # 네이버 api로 얻어온 정보 중에서 제목 부분 추출 #\n",
    "                        link = v.get('link')        # 네이버 api로 얻어온 정보 중에서 link 부분 추출 #\n",
    "\n",
    "                        pub_date = v.get('pubDate')     # 네이버 api로 얻어온 정보중에서 날짜 부분 추출 #\n",
    "                        pub_date = pub_date.split()[:4] # 요일, 일, 월, 년도, 시간 부분만 추출 #\n",
    "                        pub_date = ' '.join(pub_date)\n",
    "\n",
    "                        # datetime 모듈로 불러온 date 데이터를 요일, 일, 월, 년도, 시간 순으로 변경하기 #\n",
    "\n",
    "                        date_obj = datetime.strptime(pub_date, '%a, %d %b %Y')\n",
    "                        date_obj = date_obj.date()\n",
    "\n",
    "                        if '//news.naver.com/main/' not in link:    # 네어버 뉴스로 연결되는 뉴스로만 데이터 가져오기 #\n",
    "                            continue\n",
    "\n",
    "                        if self.date != date_obj or self.count+1 > 100:   # 네이버 api로 얻어온 정보의 날짜가 오늘 날짜와 다르면 끝내기 #\n",
    "                            is_stop = True\n",
    "                            break\n",
    "                        self.count += 1\n",
    "                        self.result[self.count] = {'link': link, 'title': title}\n",
    "                        \n",
    "                if is_stop:\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # 키워드를 입력했을 때 검색된 결과가 없을 경우 404를 리턴함 #\n",
    "    \n",
    "    def news_none(self):     \n",
    "        if len(self.result) == 0:\n",
    "            return  404\n",
    "\n",
    "\n",
    "    def save_to_file(self):\n",
    "\n",
    "        # 데이터 json 파일로 저장하기 #\n",
    "        \n",
    "        print(\"*\"*100)\n",
    "        print(\"\\n뉴스 기사 제목과 링크 크롤링이 완료되었습니다. 잠시만 기다려주세요.\")\n",
    "\n",
    "\n",
    "        json_object1 = json.dumps(self.result, ensure_ascii = False)\n",
    "        with codecs.open(self.filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(json_object1, f, ensure_ascii = False)\n",
    "\n",
    "        json_object2 = json.dumps(self.navertest, ensure_ascii = False)\n",
    "        with codecs.open(self.testfile, 'w', encoding='utf-8') as f:\n",
    "            json.dump(json_object2, f, ensure_ascii = False)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 인터넷을 통한 데이터 획득 - 2/2\n",
    "\n",
    "#### 2번 과정)   1 에서 저장한 데이터를 불러와 url에서 뉴스 기사 부분만 추출해 json 파일로 저장\n",
    "    1번 과정을 통해 얻은 기사 번호와 제목, url이 있는 json 파일을 불러와 검색된 기사를 하나씩 읽어드린다. 기사 내용이 있는 부분을 파싱하기 위해 네이버 뉴스의 기사 내용 부분의 공통된 태그를 찾아 파싱한다. 대부분의 뉴스의 기사 내용 부분의 태그는 #articleBodyContents 이고, 사진이 포함된 뉴스의 기사 내용 부분의 태그는 #articeBody 이다. 파싱한 뉴스의 기사 내용 부분을 딕셔너리로 만들어 final_result.json 파일로 저장한다.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) 1에서 저장한 데이터를 불러와 url에서 뉴스 기사 부분만 추출해 json 파일로 저장 #\n",
    "\n",
    "import json\n",
    "import codecs\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class ContentCollect:\n",
    "    def __init__(self):\n",
    "        self.filename = './naver_api_result.json'\n",
    "        f = open(self.filename, encoding='utf-8')   # 1에서 얻은 뉴스 기사들의 번호, 제목, url가 저장된 json 파일 열기 #\n",
    "        self.temp = json.loads(json.load(f))\n",
    "        self.content = dict()\n",
    "        self.file_name = './final_result.json'\n",
    "        self.count = 0\n",
    "        \n",
    "\n",
    "    def run(self):\n",
    "        for i, v in self.temp.items():\n",
    "            link = v.get('link')\n",
    "            request_headers = { \n",
    "            'User-Agent' : ('Mozilla/5.0 (Windows NT 10.0;Win64; x64)\\\n",
    "            AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98\\\n",
    "            Safari/537.36'), } \n",
    "\n",
    "            try:\n",
    "                response = requests.get(link, headers=request_headers)  # 저장된 link 불러오기 #\n",
    "            except:\n",
    "                print('*' * 100, link)\n",
    "                continue \n",
    "            \n",
    "            # link에 들어가서 뉴스 기사 내용 있는 부분만 파싱 #\n",
    "            \n",
    "            soup  = BeautifulSoup(response.content, 'html.parser', from_encoding='utf-8')\n",
    "            response.encoding = None\n",
    "            main = soup.select('#articleBodyContents')\n",
    "\n",
    "            if main == []:\n",
    "                main = soup.select('#articeBody')\n",
    "            text = main[0].text\n",
    "            self.content[i] = text\n",
    "    \n",
    "    def save_to_file(self):\n",
    "        \n",
    "        # 뉴스 기사 내용 부분만 json 파일로 저장 #\n",
    "        \n",
    "        print(\"*\"*100)\n",
    "        print(\"\\n뉴스 기사 내용 크롤링이 완료되었습니다. 잠시만 기다려주세요.\")\n",
    "        \n",
    "        json_object = json.dumps(self.content, ensure_ascii = False)\n",
    "        with codecs.open(self.file_name, 'w', encoding='utf-8') as f:\n",
    "            json.dump(json_object, f, ensure_ascii = False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 분석을 위한 데이터의 가공 - 1/2\n",
    "      \n",
    "#### 3번 과정)   2에서 추출한 뉴스 기사들을 형태소로 나눈 뒤 명사 따로 json 파일로 저장\n",
    "    2번 과정을 통해 얻는 뉴스의 기사 내용 부분을 불러와 konlpy를 이용하여 형태소 분석을 한다. konlpy 라이브러리가 설치되어 있지 않으면 설치되어 있지 않다는 에러 메시지를 출력하고 404라는 에러 코드를 리턴한다. konlpy에 있는 kkma 클래스를 이용해 뉴스 기사에 있는 명사만 추출한다. 뉴스 번호와 뉴스에 있는 명사를 딕셔너리로 만들어서 형태소 분리 후 기사에 있는 명사.json으로 저장한다.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) 2에서 추출한 뉴스 기사들을 형태소로 나눈 뒤 명사 따로 json 파일로 저장 #\n",
    "\n",
    "from konlpy.tag import Kkma\n",
    "import json\n",
    "import codecs\n",
    "\n",
    "class MorphemeAnalysis:\n",
    "    def __init__(self):\n",
    "        self.openfilename = './final_result.json'\n",
    "        f = open(self.openfilename, encoding='utf-8')     # 2에서 얻은 뉴스 기사 내용 부분만 저장된 json 파일 열기 #\n",
    "        self.temp = json.loads(json.load(f))\n",
    "        self.filename = './형태소 분리 후 기사에 있는 명사.json'\n",
    "        self.kkma = None\n",
    "        self.noun_dict = dict()     # 기사에 있는 명사가 모여있는 딕셔너리 #\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        noun_list = []\n",
    "        try:\n",
    "            self.kkma = Kkma()\n",
    "        except:\n",
    "            print(\"*\"*100)\n",
    "            print(\"Konlpy 라이브러리가 설치되어있지 않습니다.\")\n",
    "            print(\"*\"*100)\n",
    "\n",
    "            return 404       \n",
    "            \n",
    "        for i, v in self.temp.items():\n",
    "            noun_list.clear()\n",
    "            morph = self.kkma.nouns(v)   # 기사에 있는 명사만 konlpy의 kkma를 이용해 추출 #\n",
    "            nouns = ' '\n",
    "      \n",
    "            for noun in morph:\n",
    "                nouns += noun + ' '\n",
    "            \n",
    "            noun_list.append(nouns)\n",
    "            self.noun_dict[i] = noun_list.copy()\n",
    "\n",
    "\n",
    "    def save_to_file(self):\n",
    "        \n",
    "        # 뉴스 기사에 있는 명사만 json 파일로 저장 #\n",
    "        \n",
    "        print(\"*\"*100)\n",
    "        print(\"\\n형태소 분석이 완료되었습니다. 잠시만 기다려주세요.\")\n",
    "        \n",
    "        json_object = json.dumps(self.noun_dict, ensure_ascii = False)\n",
    "        with codecs.open(self.filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(json_object, f, ensure_ascii = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 분석을 위한 데이터의 가공 - 2/2\n",
    "      \n",
    "#### 4번 과정)  TF-IDF를 이용해 기사끼리 유사도 검사하기\n",
    "    3번 과정을 통해 얻은 기사에 들어 있는 명사만 저장한 파일을 불러와 전체 뉴스 기사의 유사도를 확인한다. TfidfVectorizer 라이브러리가 설치죄어 있지 않으면 설지되어 있지 않다는 에러 메시지를 출력하고 404라는 에러 코드를 리턴한다. TF-IDF를 이용해 유사도를 검증한다. TF-IDF는 여러 문서로 이루어진 문서군이 있을 때, 문서의 수와 그 문서에 나타난 단어의 빈도수를 분석하여 유사도를 나타낸다. 유사도는 0에서 1 사이의 값을 가진다. 0이면 아예 다른 기사이고, 1이면 완전히 유사한 기사이다. TF-IDF를 통해 나온 결과를 TF-IDF.xlsx으로 저장한다.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) TF-IDF를 이용해 기사끼리 유사도 검사하기 #\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "class TF_IDF:\n",
    "    def __init__(self):\n",
    "        self.openfilename = './형태소 분리 후 기사에 있는 명사.json'\n",
    "        f = open(self.openfilename, encoding='utf-8')   # 3에서 얻은 뉴스 기사에 있는 명사만 저장한 json 파일 열기 #\n",
    "        self.temp = json.loads(json.load(f))\n",
    "        self.TF_noun_list = []\n",
    "        \n",
    "\n",
    "    def run(self):\n",
    "        for i, v in self.temp.items():\n",
    "            self.TF_noun_list.append(v[0])         \n",
    "        try:\n",
    "            tfidf_vectorizer = TfidfVectorizer(min_df=1)    # min-df는 최소 빈도값을 설정해주는 파라미터 #\n",
    "\n",
    "            # 문장에서 노출되는 feature(특징이 될만한 단어) 수를 합한 Document Term Matrix(문서 단어 행렬) 을 리턴 #\n",
    "\n",
    "            tfidf_matrix = tfidf_vectorizer.fit_transform(self.TF_noun_list)\n",
    "\n",
    "            document_distances = (tfidf_matrix * tfidf_matrix.T)\n",
    "            \n",
    "            # 유사도 결과는 0에서 1 사이의 값이 나옴 (아예 다르면 0, 똑같으면 1이 나옴) #\n",
    "\n",
    "            # 기사끼리의 유사도를 excel 파일로 저장 #\n",
    "            \n",
    "            print(\"*\"*100)\n",
    "            print(\"\\n뉴스 기사 유사도 검사가 완료되었습니다. 잠시만 기다려주세요\")\n",
    "            \n",
    "            df = pd.DataFrame(document_distances.toarray())\n",
    "            df.to_excel('TF-IDF.xlsx', sheet_name= 'Sheet1')\n",
    "\n",
    "        except:\n",
    "            print(\"*\"*100)\n",
    "            print(\"TfidfVectorizer 라이브러리가 설치되어있지 않습니다.\")\n",
    "            print(\"*\"*100)\n",
    "            return 404\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 분석 결과 도출 - 1/2\n",
    "    \n",
    "#### 5번 과정)   선택한 기사와 용인되는 수준의 표절률을 가진 기사 추출하기\n",
    "    4번 과정을 통해 얻은 유사도가 저장된 xlsx파일을 불러온다. 사용자에게 뉴스 기사의 번호와 뉴스 기사의 제목을 출력해 보여준다. 사용자가 유사도를 검사할 뉴스 기사의 번호를 입력받고 용인되는 수준의 유사도 또한 입력받는다. xlsx에 에서 입력받은 번호의 행을 읽어온다. 유사도 값이 0에서 1 사이의 값이므로 100을 곱해서 유사도 결과를 백분위로 나타낸다. 용인되는 수준의 유사도 이상인 유사도 결과를 가진 뉴스를 출력해준다.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5) 선택한 기사와 용인되는 수준의 표절률을 가진 기사 추출하기 #\n",
    "\n",
    "import json\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "class Similarity:\n",
    "    def __init__(self):\n",
    "        self.openfilename1 = './naver_api_result.json'\n",
    "        f = open(self.openfilename1, encoding='utf-8')      # 1에서 얻은 뉴스 기사들의 번호, 제목, url가 저장된 json 파일 열기 #\n",
    "        self.temp = json.loads(json.load(f))\n",
    "        self.wb = load_workbook(filename = 'TF-IDF.xlsx')      # 4에서 얻은 기사끼리의 유사도만 저장한 json 파일 열기 #\n",
    "        self.ws = self.wb[self.wb.sheetnames[0]]\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        num_link_dict = dict()\n",
    "\n",
    "        # 뉴스 기사의 번호와 제목을 딕셔너리로 저장 #\n",
    "        for i, v in self.temp.items(): \n",
    "            num_link_dict[i] = v.get('title')\n",
    "            print(i, v.get('title'))\n",
    "\n",
    "        object_num = int(input(\"유사도 검사를 진행할 1에서 {0} 사이의 뉴스 기사의 번호를 입력하세요 >> \".format(len(self.temp))))\n",
    "        compare_num = int(input(\"용인되는 수준의 유사도는 몇 퍼센트 인가요? >> \"))\n",
    "        print(\"*\"*100)\n",
    "\n",
    "        similarity_percentage = []\n",
    "        compare_object = num_link_dict[str(object_num)]     # 유사도 검사를 진행할 뉴스 기사 제목 #\n",
    "\n",
    "        #유사도 검사를 진행할 뉴스 기사와 다른 기사들의 유사도 검사 결과가 저장된 행 불러오기#\n",
    "        \n",
    "        row = self.ws['{0}'.format(object_num + 1)]\n",
    "        \n",
    "        for cell in row:\n",
    "            percentage = cell.value * 100     # 유사도 결과를 백분위로 나타내기 #\n",
    "            similarity_percentage.append(percentage)\n",
    "        clear = similarity_percentage.pop(0)    # 뉴스 기사의 번호가 있던 [0]의 요소 없애기#\n",
    "\n",
    "        display_list = []\n",
    "        for i, compare in enumerate(similarity_percentage):\n",
    "            if compare >= compare_num:      # 용인되는 수준의 유사도 이상인 유사도 결과를 가진 뉴스 기사 추출하기 #\n",
    "                if i+1 == object_num:       # 같은 기사의 유사도 결과는 지나가기 #\n",
    "                    pass\n",
    "                else:\n",
    "                    display_list.append([compare_object, num_link_dict[str(i+1)] , similarity_percentage[i]])\n",
    "\n",
    "\n",
    "        if len(display_list) > 0:\n",
    "            for i in range(len(display_list)):\n",
    "                print(\"제목이 {0}인 뉴스 기사와 제목이 {1}인 뉴스 기사의 유사도는 {2}% 입니다.\".format(display_list[i][0], display_list[i][1], display_list[i][2]))\n",
    "            print(\"용인되는 유사도를 넘어선 기사는 총 {0}개 입니다.\".format(len(display_list)))\n",
    "        else:\n",
    "            print(\"제목이 {0}인 뉴스 기사와 유사도가 {1}% 이상인 뉴스 기사는 없습니다.\".format(compare_object, compare_num))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 분석 결과 도출 - 2/2\n",
    "    \n",
    "    main 함수를 이용해 1번 과정부터 5번 과정이 이루어지는 클래스 객체를 생성한다. 각 과정의 순서에 맞게 호출한다. 1번 과정부터 5번 과정까지 순서대로 실행되고 결과를 확인할 수 있다. \n",
    "    \n",
    "    과정 중간중간 에러 코드인 404가 출력될 경우 에러 메시지를 출력한 뒤 진행을 종료한다.\n",
    "    진행이 종료되는 경우 1) 입력된 키워드로 검색된 오늘 뉴스 기사가 없는 경우\n",
    "    진행이 종료되는 경우 2) konlpy 라이브러리가 설치되어 있지 않은 경우\n",
    "    진행이 종료되는 경우 3) TfidfVectorizer 라이브러리가 설치되어 있지 않은 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "키워드를 입력하세요 ex) 코로나, 백신, 유재석, 주식, 코인 >>  주식\n",
      "****************************************************************************************************\n",
      "\n",
      "뉴스 기사 제목과 링크 크롤링이 완료되었습니다. 잠시만 기다려주세요.\n",
      "****************************************************************************************************\n",
      "\n",
      "뉴스 기사 내용 크롤링이 완료되었습니다. 잠시만 기다려주세요.\n",
      "****************************************************************************************************\n",
      "\n",
      "형태소 분석이 완료되었습니다. 잠시만 기다려주세요.\n",
      "****************************************************************************************************\n",
      "\n",
      "뉴스 기사 유사도 검사가 완료되었습니다. 잠시만 기다려주세요\n",
      "1 네이버 노조 “숨진 직원, 괴롭힘·과로…회사가 묵인하고 방조”\n",
      "2 주가에 날개 달린 두산중공업..'신중 투자' 경계 나오는 까닭은?\n",
      "3 맛과 쉼이 공존하는 공간에서 나눔을 실천하다, 이학순베이커리 이학순 대표\n",
      "4 리서치알음 &quot;1년간 상장사 <b>주식</b> 투자 수익률 -6.8%&quot;\n",
      "5 유진투자, 美프리·애프터마켓 거래서비스\n",
      "6 `카카오손보` 이번주 결정… 빅테크 첫 보험진출 이뤄질까\n",
      "7 '카카오손보' 예비허가 9일 결정…빅테크 보험업 진출 성사되나\n",
      "8 日·필리핀 이중국적 사소 US여자오픈 우승에 일본 환호\n",
      "9 3년전 대법 판결 뒤집혔다…'日징용 소송' 각하\n",
      "10 최대규모 강제징용 손배소, 1심서 원고패소…&quot;인용하면 국제법 위반&quot;(재종합)\n",
      "11 ‘비즈니스 리뷰’ 1주년, 월가 역사 돌아본다\n",
      "12 7일 장 마감 후 주요 종목뉴스\n",
      "13 국민 47% &quot;여유자금 있으면 부동산 투자&quot;…아파트 선호\n",
      "14 '日징용 소송' 각하 파장…&quot;법원이 왜 국익 따지나&quot; 성토\n",
      "15 사상 최고점 돌파한 코스피···“추가 상승 제한” vs “3630 간다”\n",
      "16 [단독]탈북학생들 책도 못 산다해 3만원 냈는데…단체가 '꿀꺽'(종합)\n",
      "17 에이씨티, 385억원에 협진기계 <b>주식</b> 양수\n",
      "18 '밈 <b>주식</b>' 광풍에 휩쓸린 서학개미… AMC·게임스톱 단타쳤다 [해외<b>주식</b> 인싸...\n",
      "19 &quot;물가·금리상승기, 리츠ETF 유망.. 종목 선택 어렵다면 섹터별 접근&quot; [해외주...\n",
      "20 [코인뉴스] 엘살바도르 법정화폐되면 비트코인 값은 오를까 내릴까\n",
      "21 원자재값·임금·추경…인플레 압력 커지는 한국\n",
      "22 <b>주식</b>·암호화폐 시장에 부는 ‘밈’ 열풍\n",
      "23 [사사건건] 송영길 “3.4% ‘1가구 1주택자’ 종부세 650억 안 깎아주는 건 합...\n",
      "24 경실련, &quot;불법 공매도 관련 정보 비공개 결정에 일관성 없어&quot;\n",
      "25 빅테크 분석서 '네이버 vs 카카오' 인기\n",
      "26 투자 중개형 ISA 두달새 60만계좌 '돌풍'\n",
      "27 에이씨티, 협진기계 인수로 HMR 시장 진출 &quot;합병 추진한다&quot;\n",
      "28 [금융위 CB 전환가액 상향 논란] &quot;CB 투자 매력 떨어져···저신용기업 자금...\n",
      "29 스컬피그 X 씬님 레깅스 신제품 '플렉스 에어 레깅스' 선보여... 3일만에 전 ...\n",
      "30 유안타證, 대만 직접 투자 ‘We Know 대만 탑티어<b>주식</b>랩’ 출시\n",
      "31 MBN[토요포커스] 김형영 한국벤처캐피탈협회 상근부회장 “국내 벤처 산업의 ...\n",
      "32 에이씨티, 협진기계 <b>주식</b> 15만5600주 양수결정\n",
      "33 드림타워카지노, 11일오픈\n",
      "34 국민 10명 중 4명은 “공시가격 현실화 그냥 하지마”\n",
      "35 공모주 ‘따상’뒤 숨겨진 불편한 진실..외국인·기관 호구된다 [최성환의 알...\n",
      "36 예금 대신 <b>주식</b>·ETF로 자산 채워···MZ세대 '재테크 계좌' 찜\n",
      "37 토스·카카오페이·P2P… 혁신금융 인허가 9일 ‘운명의 날’\n",
      "38 오늘의 증시 메모[6월 8일]\n",
      "39 그림·음악을 쪼개 판다…MZ세대의 새로운 재테크 ‘조각투자’\n",
      "40 `두슬라` 이름값한 두산중공업…&quot;천장도 바닥도 없다&quot; [박해린의 뉴스&amp;마켓]\n",
      "41 <b>주식</b>·코인 오르자 &quot;부자됐다&quot;…&quot;오늘이 가장 싸다&quot; 샤넬 단타\n",
      "42 쿠팡, 해외직구 거래 데이터 관세청과 공유 한다\n",
      "43 “코스피 3700 간다… IT·화장품·반도체·자동차株 실적장 주도”\n",
      "44 일본 기업 책임 인정한 대법 전합 판결 부정…법조계 &quot;매우 이례적&quot;\n",
      "45 코인따라 울고 웃는 '빗썸'…가상화폐 급락에 지분가치 뚝\n",
      "46 오늘의 증시 일정 (6월 8일)\n",
      "47 &quot;홍콩 주차공간 1칸이 14억&quot;…강남부자들 이런 데 투자한다 [김보미의 뉴스카...\n",
      "48 금융권'메기'된 카카오, 손해보험까지 진출할까\n",
      "49 이익 느는데 덜오른 섹터 1순위는 '반도체'\n",
      "50 ‘한국의 워렌 버핏’ 강방천 “삼성전자 대신 카카오·현대모비스”\n",
      "51 에코세대 2명중 1명 &quot;여유자금 있을 땐 아파트 투자&quot;\n",
      "52 빅데이터·AI 인재 끌어모으는 국부펀드들\n",
      "53 [매경경영지원본부] 방치하면 불이익 커지는 가지급금, 어떻게 해결할까?\n",
      "54 힐스톤파트너스, 블록체인 기술 기업 어뎁트 인수\n",
      "55 카카오손보·토스뱅크…새로운 '금융 메기'들이 온다\n",
      "56 국민銀, 인니 1.6조 손배소 리스크 해소\n",
      "57 '지금은 빅테크 금융시대'...은행 시작으로 증권·보험으로\n",
      "58 &quot;韓기술 유출 기우… 글로벌 투자유치 기회&quot;\n",
      "59 케이에이치미디어, 아이에이치큐 지분 32.55→32.31%\n",
      "60 '달러구트… ' '미드나잇… ' 베스트셀러 판타지 속에서 '가상의 만족' 찾았다\n",
      "61 [이슈진단] 中인플레이션 우려 제한적…성장주에 유리\n",
      "62 5월에 10조 순매도한 외국인…6월에는 달라질까?\n",
      "63 쿠팡 물류센터 노조 출범 &quot;열악한 현실 싸워서 바꿀 것&quot;\n",
      "64 DB금투, 투자자문 서비스 강화한다\n",
      "65 [장외<b>주식</b>] 싸이버로지텍 이틀 연속 상승\n",
      "66 공정위 &quot;친족분리 ‘꼼수’ 막고, 유연한 임원독립제 활성화&quot;\n",
      "67 세금 32조원 더 걷힐 듯…“기재부가 ‘재정 확대’ 방해한 꼴”\n",
      "68 &quot;당국 방침에 놀랐나&quot;…중금리 대출 '확' 늘리겠다는 카뱅[김대훈의 뱅크앤뱅...\n",
      "69 이월드, 상환전환우선주 288만주 보통주 전환\n",
      "70 '사상 최고' 새 역사 쓴 코스피…&quot;더 오른다&quot; vs &quot;박스에 갇힌다&quot;\n",
      "71 [마켓인사이트]국도화학, 유·무상증자 동시 추진\n",
      "72 관세청-쿠팡, 전자상거래 통관체계 효율화 업무협약\n",
      "73 코스피 최고치 경신 3252.12 마감\n",
      "74 NH투자, 유튜브 생방으로 하반기 <b>주식</b>시장 전망·포트폴리오 전략 제시\n",
      "75 [해인싸]밈 <b>주식</b> 단타매매 나선 서학개미들..급등장에 아찔한 베팅\n",
      "76 일본증시, 美증시 영향에 상승 마감…닛케이 0.27%↑\n",
      "77 경실련 &quot;외국인 불법 공매도 차단 대책 절실&quot;\n",
      "78 “도박장이면 어때?” 광란의 암호화폐 시장\n",
      "79 '日징용' 소송 각하…3년전 대법 판결 뒤집은 법원, 왜?\n",
      "80 공정위, 친족분리 악용 '꼼수' 차단\n",
      "81 공정위, 친족분리 통한 규제 꼼수에 감시망 확대...LG·LS·SK 겨냥\n",
      "82 MBN[토요포커스] 김형영 한국벤처캐피탈협회 상근부회장 “국내 벤처 산업의 ...\n",
      "83 피할 수 없는 종부세 폭탄, 배우자에게 半증여 해보세요\n",
      "84 코스피 3,250선 첫 돌파…사상 최고치 경신\n",
      "85 공정위, ‘친족분리' 악용 일감 몰아주기 ‘꼼수' 차단한다\n",
      "86 리더스트레이딩 업계 최초 ISO9001 품질경영시스템 인증 획득\n",
      "87 '기부금 꿀꺽'…비영리단체 행세 <b>주식</b>회사, 사기 혐의 수사 중\n",
      "88 유상증자 대거 몰린 6~7월···'<b>주식</b> 바겐세일' 노려볼까\n",
      "89 강다니엘의 화장품 메르넬…썸머라인 화장품 7종 출시 및 기념 이벤트\n",
      "90 비츠로시스, 30억원 규모 3자배정 유상증자 결정\n",
      "91 업황 좋은데 산은 CB 물량 부담? ···연일 미끄러지는 '흠슬라'\n",
      "92 힘 못쓰는 제약·바이오…하반기 저가매수 유효할까\n",
      "93 신촌도 20억… ‘마래푸’ 넘어선 서부선 효과\n",
      "94 김정훈 대주 사장, '어린이 교통안전 릴레이 챌린지' 동참\n",
      "95 장외 시장 최고 인기 종목은 ‘카카오뱅크·크래프톤'\n",
      "96 스컬피그-씬님, 신제품 ‘플렉스 에어 레깅스’ 6월 2일 발매···출시 3일만...\n",
      "97 미 고용지표 부진에 원/달러 환율 하락…1,112.9원 마감\n",
      "98 ‘스컬피그x유튜버 씬님’ 플렉스 에어 레깅스, 출시 3일만 품절 기록\n",
      "99 [fn마켓워치] DB금투, 투자자문 서비스 강화\n",
      "100 비트코인 4천100만원대서 횡보…이더리움 320만원 근접\n",
      "유사도 검사를 진행할 1에서 100 사이의 뉴스 기사의 번호를 입력하세요 >> 70\n",
      "용인되는 수준의 유사도는 몇 퍼센트 인가요? >> 40\n",
      "****************************************************************************************************\n",
      "제목이 '사상 최고' 새 역사 쓴 코스피…&quot;더 오른다&quot; vs &quot;박스에 갇힌다&quot;인 뉴스 기사와 유사도가 40% 이상인 뉴스 기사는 없습니다.\n"
     ]
    }
   ],
   "source": [
    "# main\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def main():\n",
    "    date = datetime.now().date()\n",
    "    search_word = input(\"키워드를 입력하세요 ex) 코로나, 백신, 유재석, 주식, 코인 >>  \")\n",
    "\n",
    "    o = LinkCollector(search_word, date)\n",
    "\n",
    "    if o.news_none() != 404:\n",
    "        \n",
    "        o2 = ContentCollect()\n",
    "        o2.run()\n",
    "        o2.save_to_file()\n",
    "    \n",
    "        o3 = MorphemeAnalysis()\n",
    "        if o3.run() != 404:\n",
    "            o3.save_to_file()\n",
    "            \n",
    "            o4 = TF_IDF()\n",
    "            if o4.run() != 404:\n",
    "                o5 = Similarity()\n",
    "                o5.run()\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass    \n",
    "    else:\n",
    "        print(\"입력된 키워드로 검색된 오늘 뉴스 기사가 없습니다. 다른 검색어를 입력하세요 \")\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) 결론\n",
    "    \n",
    "    이 프로그램을 실행하면 사용자가 입력한 키워드로 검색된 뉴스 기사들을 확인할 수 있다. 검색된 뉴스 기사들의 제목을 보고 하나의 뉴스 기사를 선택하고 용인할 수 있는 유사도 값을 입력한다. 그러면 프로그램 실행 결과로 선택한 하나의 뉴스 기사와 용인할 수 있는 유사도 값을 넘어선 뉴스 기사들의 제목과 유사도가 출력된다. \n",
    "    프로그램 결과를 통해 기사 내용이 비슷한 기사들을 확인할 수 있고 뉴스를 통해 정보를 얻을 때 비슷한 뉴스 기사는 한 번만 확인하여 더욱 빠르고 정확한 정보를 얻을 수 있게 된다. 어떤 키워드를 입력하느냐에 따라 뉴스 기사들의 유사도 값이 달라지지만 현재 이슈가 되는 코로나, 백신과 같은 키워드 또는 연예계, 정치, 주식과 관련된 키워드를 입력 했을때 기사들의 유사도가 높게 나왔다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) 참고문헌\n",
    "    \n",
    "    출처: 네이버 api 호출 예제\n",
    "          박은정, 조성준, 'KoNLPy: 쉽고 간결한 한국어 정보처리 파이썬 패키지', 제 26회 한글 및 한국어 정보처리 학술대회 논문집, 2014.\n",
    "          유원준, 딥러닝을 이용한 자연어 처리 입문, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (8) 별첨 (생성된 파일)\n",
    "    naver_api_original_data.json : 네이버 api를 이용해 획득한 데이터 원본\n",
    "    naver_api_result.json : 가공된 네이버 api 데이터\n",
    "    final_result.json : 크롤링을 이용해 획득한 기사 본문 원본\n",
    "    형태소 분리 후 기사에 있는 명사.json : 가공된 기사 본문 데이터 \n",
    "    TF-IDF.xlsx : 가공된 기사 본문 데이터를 가지고 검사한 유사도 결과 데이터"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
